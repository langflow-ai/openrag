{
  "data": {
    "id": "IBMwatsonxModel-jA4Nw",
    "node": {
      "base_classes": [
        "LanguageModel",
        "Message"
      ],
      "beta": false,
      "conditional_paths": [],
      "custom_fields": {},
      "description": "Generate text using IBM watsonx.ai foundation models.",
      "display_name": "IBM watsonx.ai",
      "documentation": "",
      "edited": false,
      "field_order": [
        "input_value",
        "system_message",
        "stream",
        "url",
        "project_id",
        "api_key",
        "model_name",
        "max_tokens",
        "stop_sequence",
        "temperature",
        "top_p",
        "frequency_penalty",
        "presence_penalty",
        "seed",
        "logprobs",
        "top_logprobs",
        "logit_bias"
      ],
      "frozen": false,
      "icon": "WatsonxAI",
      "last_updated": "2025-09-22T20:03:31.248Z",
      "legacy": false,
      "metadata": {
        "code_hash": "7767fd69a954",
        "dependencies": {
          "dependencies": [
            {
              "name": "requests",
              "version": "2.32.5"
            },
            {
              "name": "langchain_ibm",
              "version": "0.3.16"
            },
            {
              "name": "pydantic",
              "version": "2.10.6"
            },
            {
              "name": "langflow",
              "version": null
            }
          ],
          "total_dependencies": 4
        },
        "keywords": [
          "model",
          "llm",
          "language model",
          "large language model"
        ],
        "module": "langflow.components.ibm.watsonx.WatsonxAIComponent"
      },
      "minimized": false,
      "output_types": [],
      "outputs": [
        {
          "allows_loop": false,
          "cache": true,
          "display_name": "Model Response",
          "group_outputs": false,
          "method": "text_response",
          "name": "text_output",
          "options": null,
          "required_inputs": null,
          "tool_mode": true,
          "types": [
            "Message"
          ],
          "value": "__UNDEFINED__"
        },
        {
          "allows_loop": false,
          "cache": true,
          "display_name": "Language Model",
          "group_outputs": false,
          "method": "build_model",
          "name": "model_output",
          "options": null,
          "required_inputs": null,
          "selected": "LanguageModel",
          "tool_mode": true,
          "types": [
            "LanguageModel"
          ],
          "value": "__UNDEFINED__"
        }
      ],
      "pinned": false,
      "template": {
        "_type": "Component",
        "api_key": {
          "_input_type": "SecretStrInput",
          "advanced": false,
          "display_name": "API Key",
          "dynamic": false,
          "info": "The API Key to use for the model.",
          "input_types": [],
          "load_from_db": true,
          "name": "api_key",
          "password": true,
          "placeholder": "",
          "required": true,
          "show": true,
          "title_case": false,
          "type": "str",
          "value": "WATSONX_API_KEY"
        },
        "code": {
          "advanced": true,
          "dynamic": true,
          "fileTypes": [],
          "file_path": "",
          "info": "",
          "list": false,
          "load_from_db": false,
          "multiline": true,
          "name": "code",
          "password": false,
          "placeholder": "",
          "required": true,
          "show": true,
          "title_case": false,
          "type": "code",
          "value": "import json\nfrom typing import Any\n\nimport requests\nfrom langchain_ibm import ChatWatsonx\nfrom pydantic.v1 import SecretStr\n\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.field_typing import LanguageModel\nfrom langflow.field_typing.range_spec import RangeSpec\nfrom langflow.inputs.inputs import BoolInput, DropdownInput, IntInput, SecretStrInput, SliderInput, StrInput\nfrom langflow.logging.logger import logger\nfrom langflow.schema.dotdict import dotdict\n\n\nclass WatsonxAIComponent(LCModelComponent):\n    display_name = \"IBM watsonx.ai\"\n    description = \"Generate text using IBM watsonx.ai foundation models.\"\n    icon = \"WatsonxAI\"\n    name = \"IBMwatsonxModel\"\n    beta = False\n\n    _default_models = [\"ibm/granite-3-2b-instruct\", \"ibm/granite-3-8b-instruct\", \"ibm/granite-13b-instruct-v2\"]\n\n    inputs = [\n        *LCModelComponent._base_inputs,\n        DropdownInput(\n            name=\"url\",\n            display_name=\"watsonx API Endpoint\",\n            info=\"The base URL of the API.\",\n            value=None,\n            options=[\n                \"https://us-south.ml.cloud.ibm.com\",\n                \"https://eu-de.ml.cloud.ibm.com\",\n                \"https://eu-gb.ml.cloud.ibm.com\",\n                \"https://au-syd.ml.cloud.ibm.com\",\n                \"https://jp-tok.ml.cloud.ibm.com\",\n                \"https://ca-tor.ml.cloud.ibm.com\",\n            ],\n            real_time_refresh=True,\n        ),\n        StrInput(\n            name=\"project_id\",\n            display_name=\"watsonx Project ID\",\n            required=True,\n            info=\"The project ID or deployment space ID that is associated with the foundation model.\",\n        ),\n        SecretStrInput(\n            name=\"api_key\",\n            display_name=\"API Key\",\n            info=\"The API Key to use for the model.\",\n            required=True,\n        ),\n        DropdownInput(\n            name=\"model_name\",\n            display_name=\"Model Name\",\n            options=[],\n            value=None,\n            dynamic=True,\n            required=True,\n        ),\n        IntInput(\n            name=\"max_tokens\",\n            display_name=\"Max Tokens\",\n            advanced=True,\n            info=\"The maximum number of tokens to generate.\",\n            range_spec=RangeSpec(min=1, max=4096),\n            value=1000,\n        ),\n        StrInput(\n            name=\"stop_sequence\",\n            display_name=\"Stop Sequence\",\n            advanced=True,\n            info=\"Sequence where generation should stop.\",\n            field_type=\"str\",\n        ),\n        SliderInput(\n            name=\"temperature\",\n            display_name=\"Temperature\",\n            info=\"Controls randomness, higher values increase diversity.\",\n            value=0.1,\n            range_spec=RangeSpec(min=0, max=2, step=0.01),\n            advanced=True,\n        ),\n        SliderInput(\n            name=\"top_p\",\n            display_name=\"Top P\",\n            info=\"The cumulative probability cutoff for token selection. \"\n            \"Lower values mean sampling from a smaller, more top-weighted nucleus.\",\n            value=0.9,\n            range_spec=RangeSpec(min=0, max=1, step=0.01),\n            advanced=True,\n        ),\n        SliderInput(\n            name=\"frequency_penalty\",\n            display_name=\"Frequency Penalty\",\n            info=\"Penalty for frequency of token usage.\",\n            value=0.5,\n            range_spec=RangeSpec(min=-2.0, max=2.0, step=0.01),\n            advanced=True,\n        ),\n        SliderInput(\n            name=\"presence_penalty\",\n            display_name=\"Presence Penalty\",\n            info=\"Penalty for token presence in prior text.\",\n            value=0.3,\n            range_spec=RangeSpec(min=-2.0, max=2.0, step=0.01),\n            advanced=True,\n        ),\n        IntInput(\n            name=\"seed\",\n            display_name=\"Random Seed\",\n            advanced=True,\n            info=\"The random seed for the model.\",\n            value=8,\n        ),\n        BoolInput(\n            name=\"logprobs\",\n            display_name=\"Log Probabilities\",\n            advanced=True,\n            info=\"Whether to return log probabilities of the output tokens.\",\n            value=True,\n        ),\n        IntInput(\n            name=\"top_logprobs\",\n            display_name=\"Top Log Probabilities\",\n            advanced=True,\n            info=\"Number of most likely tokens to return at each position.\",\n            value=3,\n            range_spec=RangeSpec(min=1, max=20),\n        ),\n        StrInput(\n            name=\"logit_bias\",\n            display_name=\"Logit Bias\",\n            advanced=True,\n            info='JSON string of token IDs to bias or suppress (e.g., {\"1003\": -100, \"1004\": 100}).',\n            field_type=\"str\",\n        ),\n    ]\n\n    @staticmethod\n    def fetch_models(base_url: str) -> list[str]:\n        \"\"\"Fetch available models from the watsonx.ai API.\"\"\"\n        try:\n            endpoint = f\"{base_url}/ml/v1/foundation_model_specs\"\n            params = {\"version\": \"2024-09-16\", \"filters\": \"function_text_chat,!lifecycle_withdrawn\"}\n            response = requests.get(endpoint, params=params, timeout=10)\n            response.raise_for_status()\n            data = response.json()\n            models = [model[\"model_id\"] for model in data.get(\"resources\", [])]\n            return sorted(models)\n        except Exception:  # noqa: BLE001\n            logger.exception(\"Error fetching models. Using default models.\")\n            return WatsonxAIComponent._default_models\n\n    def update_build_config(self, build_config: dotdict, field_value: Any, field_name: str | None = None):\n        \"\"\"Update model options when URL or API key changes.\"\"\"\n        logger.info(\"Updating build config. Field name: %s, Field value: %s\", field_name, field_value)\n\n        if field_name == \"url\" and field_value:\n            try:\n                models = self.fetch_models(base_url=build_config.url.value)\n                build_config.model_name.options = models\n                if build_config.model_name.value:\n                    build_config.model_name.value = models[0]\n                info_message = f\"Updated model options: {len(models)} models found in {build_config.url.value}\"\n                logger.info(info_message)\n            except Exception:  # noqa: BLE001\n                logger.exception(\"Error updating model options.\")\n\n    def build_model(self) -> LanguageModel:\n        # Parse logit_bias from JSON string if provided\n        logit_bias = None\n        if hasattr(self, \"logit_bias\") and self.logit_bias:\n            try:\n                logit_bias = json.loads(self.logit_bias)\n            except json.JSONDecodeError:\n                logger.warning(\"Invalid logit_bias JSON format. Using default instead.\")\n                logit_bias = {\"1003\": -100, \"1004\": -100}\n\n        chat_params = {\n            \"max_tokens\": getattr(self, \"max_tokens\", None),\n            \"temperature\": getattr(self, \"temperature\", None),\n            \"top_p\": getattr(self, \"top_p\", None),\n            \"frequency_penalty\": getattr(self, \"frequency_penalty\", None),\n            \"presence_penalty\": getattr(self, \"presence_penalty\", None),\n            \"seed\": getattr(self, \"seed\", None),\n            \"stop\": [self.stop_sequence] if self.stop_sequence else [],\n            \"n\": 1,\n            \"logprobs\": getattr(self, \"logprobs\", True),\n            \"top_logprobs\": getattr(self, \"top_logprobs\", None),\n            \"time_limit\": 600000,\n            \"logit_bias\": logit_bias,\n        }\n\n        return ChatWatsonx(\n            apikey=SecretStr(self.api_key).get_secret_value(),\n            url=self.url,\n            project_id=self.project_id,\n            model_id=self.model_name,\n            params=chat_params,\n            streaming=self.stream,\n        )\n"
        },
        "frequency_penalty": {
          "_input_type": "SliderInput",
          "advanced": true,
          "display_name": "Frequency Penalty",
          "dynamic": false,
          "info": "Penalty for frequency of token usage.",
          "max_label": "",
          "max_label_icon": "",
          "min_label": "",
          "min_label_icon": "",
          "name": "frequency_penalty",
          "placeholder": "",
          "range_spec": {
            "max": 2,
            "min": -2,
            "step": 0.01,
            "step_type": "float"
          },
          "required": false,
          "show": true,
          "slider_buttons": false,
          "slider_buttons_options": [],
          "slider_input": false,
          "title_case": false,
          "tool_mode": false,
          "type": "slider",
          "value": 0.5
        },
        "input_value": {
          "_input_type": "MessageInput",
          "advanced": false,
          "display_name": "Input",
          "dynamic": false,
          "info": "",
          "input_types": [
            "Message"
          ],
          "list": false,
          "list_add_label": "Add More",
          "load_from_db": false,
          "name": "input_value",
          "placeholder": "",
          "required": false,
          "show": true,
          "title_case": false,
          "tool_mode": false,
          "trace_as_input": true,
          "trace_as_metadata": true,
          "type": "str",
          "value": ""
        },
        "logit_bias": {
          "_input_type": "StrInput",
          "advanced": true,
          "display_name": "Logit Bias",
          "dynamic": false,
          "info": "JSON string of token IDs to bias or suppress (e.g., {\"1003\": -100, \"1004\": 100}).",
          "list": false,
          "list_add_label": "Add More",
          "load_from_db": false,
          "name": "logit_bias",
          "placeholder": "",
          "required": false,
          "show": true,
          "title_case": false,
          "tool_mode": false,
          "trace_as_metadata": true,
          "type": "str",
          "value": ""
        },
        "logprobs": {
          "_input_type": "BoolInput",
          "advanced": true,
          "display_name": "Log Probabilities",
          "dynamic": false,
          "info": "Whether to return log probabilities of the output tokens.",
          "list": false,
          "list_add_label": "Add More",
          "name": "logprobs",
          "placeholder": "",
          "required": false,
          "show": true,
          "title_case": false,
          "tool_mode": false,
          "trace_as_metadata": true,
          "type": "bool",
          "value": true
        },
        "max_tokens": {
          "_input_type": "IntInput",
          "advanced": true,
          "display_name": "Max Tokens",
          "dynamic": false,
          "info": "The maximum number of tokens to generate.",
          "list": false,
          "list_add_label": "Add More",
          "name": "max_tokens",
          "placeholder": "",
          "range_spec": {
            "max": 4096,
            "min": 1,
            "step": 0.1,
            "step_type": "float"
          },
          "required": false,
          "show": true,
          "title_case": false,
          "tool_mode": false,
          "trace_as_metadata": true,
          "type": "int",
          "value": 1000
        },
        "model_name": {
          "_input_type": "DropdownInput",
          "advanced": false,
          "combobox": false,
          "dialog_inputs": {},
          "display_name": "Model Name",
          "dynamic": true,
          "info": "",
          "name": "model_name",
          "options": [
            "ibm/granite-3-2-8b-instruct",
            "ibm/granite-3-2b-instruct",
            "ibm/granite-3-3-8b-instruct",
            "ibm/granite-3-8b-instruct",
            "ibm/granite-guardian-3-2b",
            "ibm/granite-guardian-3-8b",
            "ibm/granite-vision-3-2-2b",
            "meta-llama/llama-3-2-11b-vision-instruct",
            "meta-llama/llama-3-2-90b-vision-instruct",
            "meta-llama/llama-3-3-70b-instruct",
            "meta-llama/llama-3-405b-instruct",
            "meta-llama/llama-4-maverick-17b-128e-instruct-fp8",
            "meta-llama/llama-guard-3-11b-vision",
            "mistralai/mistral-large",
            "mistralai/mistral-medium-2505",
            "mistralai/mistral-small-3-1-24b-instruct-2503",
            "mistralai/pixtral-12b",
            "openai/gpt-oss-120b"
          ],
          "options_metadata": [],
          "placeholder": "",
          "required": true,
          "show": true,
          "title_case": false,
          "toggle": false,
          "tool_mode": false,
          "trace_as_metadata": true,
          "type": "str",
          "value": "ibm/granite-3-2-8b-instruct"
        },
        "presence_penalty": {
          "_input_type": "SliderInput",
          "advanced": true,
          "display_name": "Presence Penalty",
          "dynamic": false,
          "info": "Penalty for token presence in prior text.",
          "max_label": "",
          "max_label_icon": "",
          "min_label": "",
          "min_label_icon": "",
          "name": "presence_penalty",
          "placeholder": "",
          "range_spec": {
            "max": 2,
            "min": -2,
            "step": 0.01,
            "step_type": "float"
          },
          "required": false,
          "show": true,
          "slider_buttons": false,
          "slider_buttons_options": [],
          "slider_input": false,
          "title_case": false,
          "tool_mode": false,
          "type": "slider",
          "value": 0.3
        },
        "project_id": {
          "_input_type": "StrInput",
          "advanced": false,
          "display_name": "watsonx Project ID",
          "dynamic": false,
          "info": "The project ID or deployment space ID that is associated with the foundation model.",
          "list": false,
          "list_add_label": "Add More",
          "load_from_db": true,
          "name": "project_id",
          "placeholder": "",
          "required": true,
          "show": true,
          "title_case": false,
          "tool_mode": false,
          "trace_as_metadata": true,
          "type": "str",
          "value": "WATSONX_PROJECT_ID"
        },
        "seed": {
          "_input_type": "IntInput",
          "advanced": true,
          "display_name": "Random Seed",
          "dynamic": false,
          "info": "The random seed for the model.",
          "list": false,
          "list_add_label": "Add More",
          "name": "seed",
          "placeholder": "",
          "required": false,
          "show": true,
          "title_case": false,
          "tool_mode": false,
          "trace_as_metadata": true,
          "type": "int",
          "value": 8
        },
        "stop_sequence": {
          "_input_type": "StrInput",
          "advanced": true,
          "display_name": "Stop Sequence",
          "dynamic": false,
          "info": "Sequence where generation should stop.",
          "list": false,
          "list_add_label": "Add More",
          "load_from_db": false,
          "name": "stop_sequence",
          "placeholder": "",
          "required": false,
          "show": true,
          "title_case": false,
          "tool_mode": false,
          "trace_as_metadata": true,
          "type": "str",
          "value": ""
        },
        "stream": {
          "_input_type": "BoolInput",
          "advanced": true,
          "display_name": "Stream",
          "dynamic": false,
          "info": "Stream the response from the model. Streaming works only in Chat.",
          "list": false,
          "list_add_label": "Add More",
          "name": "stream",
          "placeholder": "",
          "required": false,
          "show": true,
          "title_case": false,
          "tool_mode": false,
          "trace_as_metadata": true,
          "type": "bool",
          "value": false
        },
        "system_message": {
          "_input_type": "MultilineInput",
          "advanced": false,
          "copy_field": false,
          "display_name": "System Message",
          "dynamic": false,
          "info": "System message to pass to the model.",
          "input_types": [
            "Message"
          ],
          "list": false,
          "list_add_label": "Add More",
          "load_from_db": false,
          "multiline": true,
          "name": "system_message",
          "placeholder": "",
          "required": false,
          "show": true,
          "title_case": false,
          "tool_mode": false,
          "trace_as_input": true,
          "trace_as_metadata": true,
          "type": "str",
          "value": ""
        },
        "temperature": {
          "_input_type": "SliderInput",
          "advanced": true,
          "display_name": "Temperature",
          "dynamic": false,
          "info": "Controls randomness, higher values increase diversity.",
          "max_label": "",
          "max_label_icon": "",
          "min_label": "",
          "min_label_icon": "",
          "name": "temperature",
          "placeholder": "",
          "range_spec": {
            "max": 2,
            "min": 0,
            "step": 0.01,
            "step_type": "float"
          },
          "required": false,
          "show": true,
          "slider_buttons": false,
          "slider_buttons_options": [],
          "slider_input": false,
          "title_case": false,
          "tool_mode": false,
          "type": "slider",
          "value": 0.1
        },
        "top_logprobs": {
          "_input_type": "IntInput",
          "advanced": true,
          "display_name": "Top Log Probabilities",
          "dynamic": false,
          "info": "Number of most likely tokens to return at each position.",
          "list": false,
          "list_add_label": "Add More",
          "name": "top_logprobs",
          "placeholder": "",
          "range_spec": {
            "max": 20,
            "min": 1,
            "step": 0.1,
            "step_type": "float"
          },
          "required": false,
          "show": true,
          "title_case": false,
          "tool_mode": false,
          "trace_as_metadata": true,
          "type": "int",
          "value": 3
        },
        "top_p": {
          "_input_type": "SliderInput",
          "advanced": true,
          "display_name": "Top P",
          "dynamic": false,
          "info": "The cumulative probability cutoff for token selection. Lower values mean sampling from a smaller, more top-weighted nucleus.",
          "max_label": "",
          "max_label_icon": "",
          "min_label": "",
          "min_label_icon": "",
          "name": "top_p",
          "placeholder": "",
          "range_spec": {
            "max": 1,
            "min": 0,
            "step": 0.01,
            "step_type": "float"
          },
          "required": false,
          "show": true,
          "slider_buttons": false,
          "slider_buttons_options": [],
          "slider_input": false,
          "title_case": false,
          "tool_mode": false,
          "type": "slider",
          "value": 0.9
        },
        "url": {
          "_input_type": "DropdownInput",
          "advanced": false,
          "combobox": false,
          "dialog_inputs": {},
          "display_name": "watsonx API Endpoint",
          "dynamic": false,
          "info": "The base URL of the API.",
          "name": "url",
          "options": [
            "https://us-south.ml.cloud.ibm.com",
            "https://eu-de.ml.cloud.ibm.com",
            "https://eu-gb.ml.cloud.ibm.com",
            "https://au-syd.ml.cloud.ibm.com",
            "https://jp-tok.ml.cloud.ibm.com",
            "https://ca-tor.ml.cloud.ibm.com"
          ],
          "options_metadata": [],
          "placeholder": "",
          "real_time_refresh": true,
          "required": false,
          "show": true,
          "title_case": false,
          "toggle": false,
          "tool_mode": false,
          "trace_as_metadata": true,
          "type": "str",
          "value": "https://us-south.ml.cloud.ibm.com"
        }
      },
      "tool_mode": false
    },
    "selected_output": "model_output",
    "showNode": true,
    "type": "IBMwatsonxModel"
  },
  "dragging": false,
  "id": "IBMwatsonxModel-jA4Nw",
  "measured": {
    "height": 632,
    "width": 320
  },
  "position": {
    "x": 371.93566807042805,
    "y": 197.47711431325635
  },
  "selected": false,
  "type": "genericNode"
}