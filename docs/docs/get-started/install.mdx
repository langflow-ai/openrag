---
title: Install OpenRAG
slug: /install
---

import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';

OpenRAG can be installed in multiple ways:

* [**Python wheel**](#install-python-wheel): Install the OpenRAG Python wheel and use the [OpenRAG Terminal User Interface (TUI)](/get-started/tui) to install, run, and configure your OpenRAG deployment without running Docker commands.

* [**Docker Compose**](#install-and-run-docker): Clone the OpenRAG repository and deploy OpenRAG with Docker Compose, including all services and dependencies.

## Prerequisites 

- [Python Version 3.10 to 3.13](https://www.python.org/downloads/release/python-3100/)
- [uv](https://docs.astral.sh/uv/getting-started/installation/)
- [Docker](https://docs.docker.com/get-docker/) or [Podman](https://podman.io/docs/installation) installed
- [Docker Compose](https://docs.docker.com/compose/install/) installed. If using Podman, use [podman-compose](https://docs.podman.io/en/latest/markdown/podman-compose.1.html) or alias Docker compose commands to Podman commands.
- For GPU support: (TBD)

## Python wheel {#install-python-wheel}

The Python wheel is currently available internally, but will be available on PyPI at launch.
The wheel installs the OpenRAG wheel, which includes the TUI for installing, running, and managing OpenRAG.
For more information on virtual environments, see [uv](https://docs.astral.sh/uv/pip/environments).

1. Create a new project with a virtual environment using [uv](https://docs.astral.sh/uv/pip/environments).

   ```bash
   uv init YOUR_PROJECT_NAME
   cd YOUR_PROJECT_NAME
   ```
2. Add the OpenRAG wheel to your project and install it in the virtual environment.
   Replace `PATH/TO/` and `VERSION` with your OpenRAG wheel location and version.
   ```bash
   uv add PATH/TO/openrag-VERSION-py3-none-any.whl
   ```
3. Ensure all dependencies are installed and updated in your virtual environment.
   ```bash
   uv sync
   ```

4. Start the OpenRAG TUI.
   ```bash
   uv run openrag
   ```

   The OpenRAG TUI opens.

5. To install OpenRAG with Basic Setup, click **Basic Setup** or press <kbd>1</kbd>. Basic Setup does not set up OAuth connections for ingestion from Google Drive, OneDrive, or AWS. For OAuth setup, see [Advanced Setup](#advanced-setup).
   The TUI prompts you for the required startup values.
   Click **Generate Passwords** to autocomplete fields that contain **Auto-generated Secure Password**, or bring your own passwords.
    <details closed>
    <summary>Where do I find the required startup values?</summary>
    
    | Variable | Where to Find | Description |
    |----------|---------------|-------------|
    | `OPENSEARCH_PASSWORD` | Auto-generated secure password | The password for OpenSearch database access. Must be at least 8 characters and must contain at least one uppercase letter, one lowercase letter, one digit, and one special character. |
    | `OPENAI_API_KEY` | [OpenAI Platform](https://platform.openai.com/api-keys) | API key from your OpenAI account. |
    | `LANGFLOW_SUPERUSER` | User generated | Username for Langflow admin access. For more, see [Langflow docs](https://docs.langflow.org/api-keys-and-authentication#langflow-superuser). |
    | `LANGFLOW_SUPERUSER_PASSWORD` | Auto-generated secure password | Password for Langflow admin access. For more, see the [Langflow docs](https://docs.langflow.org/api-keys-and-authentication#langflow-superuser). |
    | `LANGFLOW_SECRET_KEY` | Auto-generated secure key | Secret key for Langflow security. For more, see the [Langflow docs](https://docs.langflow.org/api-keys-and-authentication#langflow-secret-key). |
    | `LANGFLOW_AUTO_LOGIN` | Auto-generated or manual | Auto-login configuration. For more, see the [Langflow docs](https://docs.langflow.org/api-keys-and-authentication#langflow-auto-login). |
    | `LANGFLOW_NEW_USER_IS_ACTIVE` | Langflow | New user activation setting. For more, see the [Langflow docs](https://docs.langflow.org/api-keys-and-authentication#langflow-new-user-is-active). |
    | `LANGFLOW_ENABLE_SUPERUSER_CLI` | Langflow server | Superuser CLI access setting. For more, see the [Langflow docs](https://docs.langflow.org/api-keys-and-authentication#langflow-enable-superuser-cli). |
    | `DOCUMENTS_PATH` | Set your local path | Path to your document storage directory. |
    
    </details>
   
   To complete credentials, click **Save Configuration**.

6. To start OpenRAG with your credentials, click **Start Container Services**.
   Startup pulls container images and starts them, so it can take some time.
   The operation has completed when the **Close** button is available and the terminal displays:
   ```bash
   Services started successfully
   Command completed successfully
   ```

7. To open the OpenRAG application, click **Open App**, press <kbd>6</kbd>, or navigate to `http://localhost:3000`.
   The application opens.
8. Select your language model and embedding model provider, and complete the required fields.
   **Your provider can only be selected once, and you must use the same provider for your language model and embedding model.
   To change your provider selection, you must restart OpenRAG and delete the `config.yml` file.**
   **Your language model can be changed, but your embeddings model cannot be changed.**

<Tabs groupId="Embedding provider">
  <TabItem value="OpenAI" label="OpenAI" default>
   9. If you already entered a value for `OPENAI_API_KEY` in the TUI in Step 5, enable **Get API key from environment variable**.
   10. Under **Advanced settings**, select your **Embedding Model** and **Language Model**.
   11. To load 2 sample PDFs, enable **Sample dataset**.
   This is recommended, but not required.
   12. Click **Complete**.

  </TabItem>
  <TabItem value="IBM" label="IBM">
   9. Complete the fields for **watsonx.ai API Endpoint**, **IBM API key**, and **IBM Project ID**.
   These values are found in your IBM watsonx deployment.
   10. Under **Advanced settings**, select your **Embedding Model** and **Language Model**.
   11. To load 2 sample PDFs, enable **Sample dataset**.
   This is recommended, but not required.
   12. Click **Complete**.

  </TabItem>
  <TabItem value="Ollama" label="Ollama">
   9. Enter your Ollama server's base URL address.
   The default Ollama server address is `http://localhost:11434`.
   Since OpenRAG is running in a container, you may need to change `localhost` to access services outside of the container. For example, change `http://localhost:11434` to `http://host.docker.internal:11434` to connect to Ollama.
   OpenRAG automatically sends a test connection to your Ollama server to confirm connectivity.
   10. Select the **Embedding Model** and **Language Model** your Ollama server is running.
   OpenRAG automatically lists the available models from your Ollama server.
   11. To load 2 sample PDFs, enable **Sample dataset**.
   This is recommended, but not required.
   12. Click **Complete**.
    
  </TabItem>
</Tabs>

8. Continue with the [Quickstart](/quickstart).

### Advanced Setup {#advanced-setup}

**Advanced Setup** includes the required values from **Basic Setup**, with additional settings for OAuth credentials.
If the OpenRAG TUI detects OAuth credentials, it enforces the Advanced Setup path.
1. Add your client and secret values for Google, Azure, or AWS OAuth.
These values can be found in your OAuth provider.
2. The OpenRAG TUI presents redirect URIs for your OAuth app.
These are the URLs your OAuth provider will redirect back to after user sign-in.
Register these redirect values with your OAuth provider as they are presented in the TUI.
3. To open the OpenRAG application, click **Open App** or press <kbd>6</kbd>.
You will be presented with your provider's OAuth sign-in screen, and be redirected to the redirect URI after sign-in.

Two additional variables are available for Advanced Setup:

The `LANGFLOW_PUBLIC_URL` controls where the Langflow web interface can be accessed. This is where users interact with their flows in a browser.

The `WEBHOOK_BASE_URL` controls where the endpoint for `/connectors/CONNECTOR_TYPE/webhook` will be available.
This connection enables real-time document synchronization with external services.
For example, for Google Drive file synchronization the webhook URL is `/connectors/google_drive/webhook`.

## Docker {#install-and-run-docker}

There are two different Docker Compose files.
They deploy the same applications and containers, but to different environments.

- [`docker-compose.yml`](https://github.com/langflow-ai/openrag/blob/main/docker-compose.yml) is an OpenRAG deployment with GPU support for accelerated AI processing.

- [`docker-compose-cpu.yml`](https://github.com/langflow-ai/openrag/blob/main/docker-compose-cpu.yml) is a CPU-only version of OpenRAG for systems without GPU support. Use this Docker compose file for environments where GPU drivers aren't available.

To install OpenRAG with Docker Compose:

1. Clone the OpenRAG repository.
   ```bash
   git clone https://github.com/langflow-ai/openrag.git
   cd openrag
   ```

2. Copy the example `.env` file that is included in the repository root.
  The example file includes all environment variables with comments to guide you in finding and setting their values.
   ```bash
   cp .env.example .env
   ```

   Alternatively, create a new `.env` file in the repository root.
   ```
   touch .env
   ```

3. Set environment variables. The Docker Compose files are populated with values from your `.env`, so the following values are **required** to be set:

   ```bash
   OPENSEARCH_PASSWORD=your_secure_password
   OPENAI_API_KEY=your_openai_api_key
   
   LANGFLOW_SUPERUSER=admin
   LANGFLOW_SUPERUSER_PASSWORD=your_langflow_password
   LANGFLOW_SECRET_KEY=your_secret_key
   ```
   For more information on configuring OpenRAG with environment variables, see [Environment variables](/configure/configuration).
   For additional configuration values, including `config.yaml`, see [Configuration](/configure/configuration).

4. Deploy OpenRAG with Docker Compose based on your deployment type.

   For GPU-enabled systems, run the following command:
   ```bash
   docker compose up -d
   ```

   For CPU-only systems, run the following command:
   ```bash
   docker compose -f docker-compose-cpu.yml up -d
   ```

   The OpenRAG Docker Compose file starts five containers:
   | Container Name | Default Address | Purpose |
   |---|---|---|
   | OpenRAG Backend | http://localhost:8000 | FastAPI server and core functionality. |
   | OpenRAG Frontend | http://localhost:3000 | React web interface for users. |
   | Langflow | http://localhost:7860 | AI workflow engine and flow management. |
   | OpenSearch | http://localhost:9200 | Vector database for document storage. |
   | OpenSearch Dashboards | http://localhost:5601 | Database administration interface. |

5. Verify installation by confirming all services are running.

   ```bash
   docker compose ps
   ```

   You can now access the application at:

   - **Frontend**: http://localhost:3000
   - **Backend API**: http://localhost:8000
   - **Langflow**: http://localhost:7860

Continue with the Quickstart.